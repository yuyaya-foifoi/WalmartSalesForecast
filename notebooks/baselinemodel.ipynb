{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fa09a8d",
   "metadata": {},
   "source": [
    "### ベースラインを作成するためのスクリプト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f8b0704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.functions import Functions\n",
    "from src.modeling import Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8c5067-6440-45cb-93fa-caeeada2ef5a",
   "metadata": {},
   "source": [
    "#### 用いる自作関数(src/functions.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0728261-ca75-40f5-80d0-1ff4db0730a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assigning features related to holidays\n",
      "        Args:\n",
      "            df : DataFrame\n",
      "        Return:\n",
      "            DataFrame\n",
      "        \n",
      "        \n",
      "convert specific col to datetime\n",
      "        Args:\n",
      "            df : DataFrame\n",
      "            col_name : str\n",
      "        Return:\n",
      "            pd.Series\n",
      "        \n",
      "        \n",
      "get diff of columns between df A and df B\n",
      "        Args:\n",
      "            A : DataFrame\n",
      "            B : DataFrame\n",
      "        Return:\n",
      "            list\n",
      "        \n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(Functions.appendHolidayFlag.__doc__)\n",
    "print(Functions.datetimeConverter.__doc__)\n",
    "print(Functions.getColumnsDiff.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a547c4",
   "metadata": {},
   "source": [
    "#### データセットの呼び出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e56e118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('../data/raw_data/features.csv')\n",
    "sales_train = pd.read_csv('../data/raw_data/train.csv')\n",
    "stores = pd.read_csv('../data/raw_data/stores.csv')\n",
    "sales_test = pd.read_csv('../data/raw_data/test.csv')\n",
    "\n",
    "# Date列の型変換\n",
    "features['Date'] = Functions.datetimeConverter(features, 'Date')\n",
    "sales_train['Date'] = Functions.datetimeConverter(sales_train, 'Date')\n",
    "sales_test['Date'] = Functions.datetimeConverter(sales_test, 'Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb437ebb",
   "metadata": {},
   "source": [
    "#### データセットの結合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0b56745",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_train = sales_train.merge(features, how=\"left\", on=[\"Store\",\"Date\",\"IsHoliday\"]).merge(stores, how=\"left\", on=[\"Store\"])\n",
    "df_all_test = sales_test.merge(features, how=\"left\", on=[\"Store\",\"Date\",\"IsHoliday\"]).merge(stores, how=\"left\", on=[\"Store\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526401bf",
   "metadata": {},
   "source": [
    "#### ラベル変更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24e51bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_train = pd.get_dummies(df_all_train, columns=[\"Type\"])\n",
    "df_all_test = pd.get_dummies(df_all_test, columns=[\"Type\"])\n",
    "\n",
    "LE = LabelEncoder()\n",
    "df_all_train['IsHoliday'] = LE.fit_transform(df_all_train['IsHoliday'])\n",
    "df_all_test['IsHoliday'] = LE.fit_transform(df_all_test['IsHoliday'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e112b3fd",
   "metadata": {},
   "source": [
    "#### 特徴量の追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "871b21c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-29f57272451f>:5: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  df_all_train[\"week\"] = df_all_train['Date'].dt.week\n",
      "<ipython-input-6-29f57272451f>:10: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  df_all_test[\"week\"] = df_all_test['Date'].dt.week\n"
     ]
    }
   ],
   "source": [
    "df_all_train = Functions.appendHolidayFlag(df_all_train)\n",
    "df_all_test = Functions.appendHolidayFlag(df_all_test)\n",
    "\n",
    "df_all_train[\"Month\"] = df_all_train['Date'].dt.month\n",
    "df_all_train[\"week\"] = df_all_train['Date'].dt.week\n",
    "df_all_train[\"Day\"] = df_all_train['Date'].dt.day\n",
    "df_all_train = df_all_train.drop([\"Date\"], axis=1)\n",
    "\n",
    "df_all_test[\"Month\"] = df_all_test['Date'].dt.month\n",
    "df_all_test[\"week\"] = df_all_test['Date'].dt.week\n",
    "df_all_test[\"Day\"] = df_all_test['Date'].dt.day\n",
    "df_all_test = df_all_test.drop([\"Date\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84895ed8",
   "metadata": {},
   "source": [
    "#### 欠損方法の方針決め(xgbの使用時に用いるデータは欠損の補完を行わない)\n",
    "- ['mean', 'median', 'zero']それぞれで欠損を補完し、validationデータで算出したWMAEを比較する\n",
    "- -> 結果的に大きな差がなかった"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c273a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(train, test, testSize=0.3):\n",
    "    \n",
    "    d = Functions.getColumnsDiff(train, test)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = Functions.getData(train, Drops=d, testSize=0.3)\n",
    "    etr = ExtraTreesRegressor(bootstrap=False, criterion=\"mse\", max_depth=None,\n",
    "                                          max_features=\"auto\", max_leaf_nodes=None,\n",
    "                                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                          min_samples_leaf=2, min_samples_split=5,\n",
    "                                          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=30,\n",
    "                                          oob_score=False, random_state=2021, warm_start=False)\n",
    "    etr.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = etr.predict(X_test)\n",
    "    Modeling.computeMetrics(X_test, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbd20701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainデータとtestデータを結合し、一括変換\n",
    "drops = Functions.getColumnsDiff(df_all_train, df_all_test)\n",
    "df_all_train_alpha = df_all_train.drop(drops, axis=1)\n",
    "df_all_train_alpha['flg'] = 'Train'\n",
    "df_all_test['flg'] = 'Test'\n",
    "stack_df = pd.concat([df_all_train_alpha, df_all_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f308b705",
   "metadata": {},
   "source": [
    "#### 欠損方法の方針決め(xgbの使用時に用いるデータは欠損の補完を行わない)\n",
    "- ['mean', 'median', 'zero']それぞれで欠損を補完し、validationデータで算出した精度を比較する\n",
    "- -> 結果的に大きな差がなかった"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17ece96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#options = ['mean', 'median', 'mode', 'zero']\n",
    "#for idx, fill in enumerate(options):\n",
    "#    if fill == 'mean':\n",
    "#        print('-'*5, 'mean', '-'*5,)\n",
    "#        import copy\n",
    "#        stack_df１ = stack_df.copy()\n",
    "#        stack_df1['MarkDown1'].fillna(stack_df1['MarkDown1'].mean(), inplace=True)\n",
    "#        stack_df1['MarkDown2'].fillna(stack_df1['MarkDown2'].mean(), inplace=True)\n",
    "#        stack_df1['MarkDown3'].fillna(stack_df1['MarkDown3'].mean(), inplace=True)\n",
    "#        stack_df1['MarkDown4'].fillna(stack_df1['MarkDown4'].mean(), inplace=True)\n",
    "#        stack_df1['MarkDown5'].fillna(stack_df1['MarkDown5'].mean(), inplace=True)\n",
    "#        stack_df1['CPI'].fillna(stack_df1['CPI'].mean(), inplace=True)\n",
    "#        stack_df1['Unemployment'].fillna(stack_df1['Unemployment'].mean(), inplace=True)\n",
    "#        \n",
    "#        train_df = stack_df1.groupby('flg').get_group('Train').drop('flg', axis=1)\n",
    "#        train_df['Weekly_Sales'] = df_all_train['Weekly_Sales']\n",
    "#        test_df = stack_df1.groupby('flg').get_group('Test').drop('flg', axis=1)\n",
    "#        \n",
    "#        compute(train_df, test_df)\n",
    "#        \n",
    "#    if fill == 'median':\n",
    "#        print('-'*5, 'median', '-'*5,)\n",
    "#        stack_df2 = stack_df.copy()\n",
    "#        stack_df2['MarkDown1'].fillna(stack_df2['MarkDown1'].median(), inplace=True)\n",
    "#        stack_df2['MarkDown2'].fillna(stack_df2['MarkDown2'].median(), inplace=True)\n",
    "#        stack_df2['MarkDown3'].fillna(stack_df2['MarkDown3'].median(), inplace=True)\n",
    "#        stack_df2['MarkDown4'].fillna(stack_df2['MarkDown4'].median(), inplace=True)\n",
    "#        stack_df2['MarkDown5'].fillna(stack_df2['MarkDown5'].median(), inplace=True)\n",
    "#        stack_df2['CPI'].fillna(stack_df2['CPI'].median(), inplace=True)\n",
    "#        stack_df2['Unemployment'].fillna(stack_df2['Unemployment'].median(), inplace=True)\n",
    "#        \n",
    "#        train_df = stack_df2.groupby('flg').get_group('Train').drop('flg', axis=1)\n",
    "#        train_df['Weekly_Sales'] = df_all_train['Weekly_Sales']\n",
    "#        test_df = stack_df2.groupby('flg').get_group('Test').drop('flg', axis=1)\n",
    "#        \n",
    "#        compute(train_df, test_df)\n",
    "#        \n",
    "#    if fill == 'zero':\n",
    "#        print('-'*5, 'zero', '-'*5,)\n",
    "#        stack_df4 = stack_df.copy()\n",
    "#        stack_df4['MarkDown1'].fillna(0, inplace=True)\n",
    "#        stack_df4['MarkDown2'].fillna(0, inplace=True)\n",
    "#        stack_df4['MarkDown3'].fillna(0, inplace=True)\n",
    "#        stack_df4['MarkDown4'].fillna(0, inplace=True)\n",
    "#        stack_df4['MarkDown5'].fillna(0, inplace=True)\n",
    "#        stack_df4['CPI'].fillna(0, inplace=True)\n",
    "#        stack_df4['Unemployment'].fillna(0, inplace=True)\n",
    "#\n",
    "#        train_df = stack_df4.groupby('flg').get_group('Train').drop('flg', axis=1)\n",
    "#        train_df['Weekly_Sales'] = df_all_train['Weekly_Sales']\n",
    "#        test_df = stack_df4.groupby('flg').get_group('Test').drop('flg', axis=1)\n",
    "#\n",
    "#        compute(train_df, test_df)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af67b22",
   "metadata": {},
   "source": [
    "### xgbについてsub用のcsvファイルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15661469",
   "metadata": {},
   "outputs": [],
   "source": [
    "drops = Functions.getColumnsDiff(df_all_train, df_all_test)\n",
    "\n",
    "X_train = df_all_train.drop(drops, axis=1)\n",
    "y_train = df_all_train['Weekly_Sales']\n",
    "\n",
    "XGB = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=2021)\n",
    "XGB.fit(X_train, y_train)\n",
    "\n",
    "y_pred = XGB.predict(df_all_test.drop('flg', axis=1))\n",
    "ss = pd.read_csv('../data/raw_data/sampleSubmission.csv')\n",
    "ss.loc[:, 'Weekly_Sales'] = y_pred\n",
    "ss.to_csv('../files/submissions/xgb_baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2232e45",
   "metadata": {},
   "source": [
    "#### ExtraTreesRegressorについてsub用のcsvファイルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea27ea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_df = stack_df.fillna(stack_df.mean())\n",
    "\n",
    "df_all_train_ = stack_df.groupby('flg').get_group('Train').drop('flg', axis=1)\n",
    "df_all_train_['Weekly_Sales'] = df_all_train['Weekly_Sales']\n",
    "df_all_test = stack_df.groupby('flg').get_group('Test').drop('flg', axis=1)\n",
    "\n",
    "drops = Functions.getColumnsDiff(df_all_train_, df_all_test)\n",
    "\n",
    "X_train = df_all_train_.drop(drops, axis=1)\n",
    "y_train = df_all_train_['Weekly_Sales']\n",
    "etr = ExtraTreesRegressor(bootstrap=False, criterion=\"mse\", max_depth=None,\n",
    "                                      max_features=\"auto\", max_leaf_nodes=None,\n",
    "                                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                      min_samples_leaf=2, min_samples_split=5,\n",
    "                                      min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=30,\n",
    "                                      oob_score=False, random_state=2021, warm_start=False)\n",
    "etr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = etr.predict(df_all_test)\n",
    "ss = pd.read_csv('../data/raw_data/sampleSubmission.csv')\n",
    "ss.loc[:, 'Weekly_Sales'] = y_pred\n",
    "ss.to_csv('../files/submissions/ExtraTreesRegressor_baseline.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
